{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GetImagenet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/1EJMHuqi6uu10KGpcv/7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDyi-lWxVMtI",
        "outputId": "fdef697b-bc8c-42b2-a4ff-1b67a292c5db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('../content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ../content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwl6wcuHVS6f"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2\n",
        "import PIL.Image\n",
        "import urllib\n",
        "from keras.models import model_from_json\n",
        "from keras.preprocessing import image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WijDN_adVSyb"
      },
      "source": [
        "# Thanks to https://colab.research.google.com/drive/1Mv7fZIRE4nByNQyE0X-jhK8NIrSWgrU3#scrollTo=iCMvyjuinmeV\n",
        "def url_to_image(url):\n",
        "\t# download the image, convert it to a NumPy array, and then read\n",
        "\t# it into OpenCV format\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\timage = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\timage = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        " \n",
        "\t# return the image\n",
        "\treturn image\n",
        "\n",
        "def get_imagenet(wnid,class_name, val_percent, out_dir = None, N_tot = None):\n",
        "  wnid = str(wnid)\n",
        "  url = 'http://www.image-net.org/api/text/imagenet.synset.geturls?wnid='+wnid\n",
        "\n",
        "  #Read through the link page\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  str_soup = str(soup)\n",
        "  #Split the links\n",
        "  split_urls=str_soup.split('\\r\\n') \n",
        "\n",
        "  if N_tot is None:\n",
        "    #If not total images to get specified, get all of them \n",
        "    N_tot = len(split_urls)\n",
        "\n",
        "  print('Total Images Intending to Import: {}'.format(N_tot))\n",
        "  \n",
        "  \n",
        "  #Training\n",
        "  N_train = int(N_tot*(1-val_percent))\n",
        "  print('Attempting to Save {} Training Images'.format(N_train))\n",
        "  train_path = str(out_dir)+'train/'+str(class_name)+'/img'\n",
        "  for progress in range(N_train):\n",
        "      if not split_urls[progress] == None:\n",
        "        try:\n",
        "          I = url_to_image(split_urls[progress])\n",
        "          if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "            #Save Image\n",
        "            save_path = train_path+str(progress)+'.jpg'\n",
        "            cv2.imwrite(save_path,I)\n",
        "        except:\n",
        "          None\n",
        "\n",
        "\n",
        "  #Validation\n",
        "  N_val = int(N_tot*val_percent)\n",
        "  print('Attempting to Save {} Validation Images'.format(N_val))\n",
        "  val_path = str(out_dir)+'validation/'+str(class_name)+'/img'\n",
        "  for progress in range(N_val):\n",
        "      if not split_urls[progress] == None:\n",
        "        try:\n",
        "          I = url_to_image(split_urls[N_train+progress])#get images that are different from the ones used for training\n",
        "          if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "            #Save Image\n",
        "            save_path = val_path+str(progress)+'.jpg'\n",
        "            cv2.imwrite(save_path,I)\n",
        "        except:\n",
        "          None\n",
        "  print('Done')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XmFsa5LVVzH"
      },
      "source": [
        "#Make dirs\n",
        "#!mkdir /content/gdrive/MyDrive/DeepLearning/DeepLearningFinal/train/great_white_shark #create the ships folder\n",
        "#!mkdir /content/gdrive/MyDrive/DeepLearning/DeepLearningFinal/validation/great_white_shark #create the ships folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkHK0pLAVXwe",
        "outputId": "a786a238-4329-4c8a-e599-286f141bda95"
      },
      "source": [
        "val_percent = 0.2\n",
        "out_dir = '/content/gdrive/MyDrive/DeepLearning/DeepLearningFinal/'\n",
        "N_per_class = 1000\n",
        "\n",
        "wnids = {'bikes': 'n03792782', \n",
        "         'ships': 'n03095699'}\n",
        "\n",
        "### MAKE THE DIRECTORIES FIRST\n",
        "for name, wnid in wnids.items():\n",
        "  get_imagenet(wnid, name, val_percent, out_dir, N_per_class)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Images Intending to Import: 1642\n",
            "Attempting to Save 1313 Training Images\n",
            "Attempting to Save328 Validation Images\n",
            "Done\n",
            "Total Images Intending to Import: 1612\n",
            "Attempting to Save 1289 Training Images\n",
            "Attempting to Save322 Validation Images\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}