{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvalofek/DefensiveLayer/blob/main/Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC0ruRQcfbvf"
      },
      "source": [
        "Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9fpi_oACPHr"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy.random as npr\n",
        "\n",
        "from tensorflow.keras.layers import Layer, InputLayer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from skimage.restoration import denoise_wavelet\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iidzu4tQFaiJ",
        "outputId": "4044b09d-d33e-439e-a4c6-8009e4943b7f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CQB9ULrCXXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ebd9fd-0b77-4374-dd14-1f3525009a77"
      },
      "source": [
        "#Original Model\n",
        "vgg = VGG19()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
            "574717952/574710816 [==============================] - 10s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3pyl4nqfdoj"
      },
      "source": [
        "Function to split the model into blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmTgTQwhE1o5"
      },
      "source": [
        "def split_model(inmodel, names):\n",
        "  #Splits model right before each of the layers in names\n",
        "  #Returns a list of the split models. \n",
        "  models = []\n",
        "  models.append(Sequential())\n",
        "\n",
        "  i = 0\n",
        "  out_size = None\n",
        "  #Adds costum layer before layer names\n",
        "  for jj, layer in enumerate(inmodel.layers):\n",
        "    if layer.name in names:\n",
        "      out_size = inmodel.layers[jj-1].output.shape[1:]\n",
        "      models.append(Sequential())\n",
        "      i +=1 \n",
        "      models[i].add(InputLayer(out_size))\n",
        "          \n",
        "    models[i].add(layer)\n",
        "    \n",
        "  return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX27RBL2F46b"
      },
      "source": [
        "#List out the vgg layer names, so it is easier for us to choose\n",
        "vgg_names = []\n",
        "for layer in vgg.layers:\n",
        "  vgg_names.append(layer.name)\n",
        "\n",
        "#Selecting layers\n",
        "# nums = [17]\n",
        "# insert_layers = [vgg_names[i] for i in nums]\n",
        "# insert_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMC0Eh3iHlOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714a2b72-96fc-478f-85c5-6986c8ebbc7c"
      },
      "source": [
        "#model_insert_positions= [[], [1], [4], [7], [12], [17]]\n",
        "model_insert_positions= [[],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],\n",
        "                         [13],[14],[15],[16],[17],[18],[19],[20],[21]]\n",
        "model_insert_names = []\n",
        "for positions in model_insert_positions:\n",
        "  model_insert_names.append([vgg_names[i] for i in positions])\n",
        "models_list = []\n",
        "for lst in model_insert_names:\n",
        "  models_list.append(split_model(vgg, lst))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTWFn61delMZ"
      },
      "source": [
        "def denoise_random_channels(intermediate, N_denoise, shp, noise_sigma =100):\r\n",
        "  #Randomly select 3 channels and denoise. Do this N_denoise times. \r\n",
        "  for jj in range(N_denoise):\r\n",
        "    selections = npr.choice(shp[3], 3)\r\n",
        "    selected = intermediate[:,:,:,selections]\r\n",
        "    denoised = denoise_wavelet(selected, multichannel=True, convert2ycbcr=True,\r\n",
        "                            method='BayesShrink', mode='soft',sigma=noise_sigma, rescale_sigma = True)\r\n",
        "    for num, selection in enumerate(selections):\r\n",
        "      intermediate[:,:,:,selection] = denoised[:,:,:,num]\r\n",
        "  #Added random component \r\n",
        "  inter = tf.convert_to_tensor(intermediate.reshape(1, shp[1], shp[2], shp[3]))\r\n",
        "  return inter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE3S9UoOf2xa"
      },
      "source": [
        "def denoise_all_channels(intermediate, shp, noise_sigma =100):\r\n",
        "  #Takes all the channels and selects an order, denoises them by threes in that \r\n",
        "  #chosen order (omitting any that are left over at the end). \r\n",
        "  channels_3 = shp[3] - shp[3]%3  #Ignore the last \r\n",
        "  order = npr.permutation(shp[3])\r\n",
        "  for jj in range(int(len(order)/3)):\r\n",
        "    selections = order[3*jj:3*jj+3]\r\n",
        "    selected = intermediate[:,:,:,selections]\r\n",
        "    denoised = denoise_wavelet(selected, multichannel=True, convert2ycbcr=True,\r\n",
        "                            method='BayesShrink', mode='soft',sigma=noise_sigma, rescale_sigma = True)\r\n",
        "    for num, selection in enumerate(selections):\r\n",
        "      intermediate[:,:,:,selection] = denoised[:,:,:,num]\r\n",
        "  #Added random component \r\n",
        "  inter = tf.convert_to_tensor(intermediate.reshape(1, shp[1], shp[2], shp[3]))\r\n",
        "  return inter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDb8KXQG9oje"
      },
      "source": [
        "# FGSM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2a0UJ-_9yT7"
      },
      "source": [
        "## Test Accuracy of Each Single Layer Insertion For Each Epsilon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeNnoILYiNnA"
      },
      "source": [
        "## ACCURACY TESTING LOOP\n",
        "N_denoise = 50\n",
        "TestingLoops = 5\n",
        "noise_sigma = 100\n",
        "\n",
        "eps_list = ['eps_0.0', 'eps_0.1', 'eps_0.3', 'eps_0.5', 'eps_1.0']\n",
        "\n",
        "#dir = \"/content/drive/MyDrive/DeepLearning/DeepLearningFinal/adversarial_images/fgsm/\"\n",
        "dir =  \"/content/drive/MyDrive/DeepLearningFinal/adversarial_images/fgsm/\"\n",
        "\n",
        "acc_save = {}\n",
        "for eps in eps_list:\n",
        "  #Initialize epselon dictionary\n",
        "  acc_save[eps] = {}\n",
        "  #Get directory\n",
        "  select_dir = dir+eps\n",
        "  #Import Images\n",
        "  val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    select_dir,\n",
        "    seed = 123,\n",
        "    shuffle=True,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=1)\n",
        "  class_names = val_ds.class_names\n",
        "\n",
        "  # extract images and labels from validation set and convert human-readable labels\n",
        "  # to ImageNet class IDs\n",
        "  images, labels = [], []\n",
        "  class_ids = {'bikes': 671, 'ships': 510}\n",
        "  for img, label in val_ds:\n",
        "    class_id = class_ids[class_names[int(label)]]\n",
        "    labels.append(class_id)\n",
        "    images.append(img)\n",
        "\n",
        "  #Iterate over the models we inserted the layers in \n",
        "  for ii, mdl in enumerate(models_list):\n",
        "    #Count the number of correct predictions\n",
        "    correct_prediction = []\n",
        "    #Run testing a couple times on the same set of images so we average out some of the randomness\n",
        "    for loop in range(TestingLoops):\n",
        "      #Loop over images\n",
        "      for idx, im in enumerate(images):\n",
        "        L = len(mdl)\n",
        "        inter = im\n",
        "        #Loop over models\n",
        "        for i, block in enumerate(mdl):\n",
        "          inter = block(inter)\n",
        "          if i != L-1:\n",
        "            #Costum layer insertion:\n",
        "\n",
        "            shp = inter.shape\n",
        "            intermediate = inter.numpy()\n",
        "            inter = denoise_all_channels(intermediate, shp)\n",
        "          else:\n",
        "            out = np.argmax(inter)\n",
        "        correct_prediction.append(labels[idx] == int(out))\n",
        "\n",
        "    #Once all the images are out, calculate the percent accuracy:\n",
        "    acc_save[eps][str(ii)] = np.mean(correct_prediction)\n",
        "    ii +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfZRvkRV-EjI"
      },
      "source": [
        "## Save and Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cohwlypKtT3P"
      },
      "source": [
        "# save accuracy to file\n",
        "with open('/content/drive/MyDrive/DeepLearningFinal/results/fgsm_accuracy.txt','a') as file:\n",
        "  file.write(\"Denoise Function: All\\n\")\n",
        "  file.write(\"Testing Loops: \" + str(TestingLoops) + \"\\n\")\n",
        "  file.write(\"N_denoise: \" + str(N_denoise) + \"\\n\")\n",
        "  file.write(\"Noise_sigma: \" + str(noise_sigma) + \"\\n\")\n",
        "  for eps,model in acc_save.items():\n",
        "    file.write(eps + \"\\n\")\n",
        "    for model_name,acc in model.items():\n",
        "      file.write(model_name + \" \" + str(acc) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaEwxHJCkgox"
      },
      "source": [
        "layer_nums = np.array(model_insert_positions[1:])\r\n",
        "#out_dir = '/content/drive/MyDrive/DeepLearning/DeepLearningFinal/results/'\r\n",
        "out_dir = '/content/drive/MyDrive/DeepLearningFinal/results/'\r\n",
        "\r\n",
        "for eps in eps_list:\r\n",
        "  plt.figure(figsize = (8,4))\r\n",
        "  accs = []\r\n",
        "  for acc in acc_save[eps].values():\r\n",
        "    accs.append(acc)\r\n",
        "    \r\n",
        "  plt.plot(layer_nums, accs[1:])\r\n",
        "  xmin, xmx = plt.xlim()\r\n",
        "  plt.hlines(accs[0], xmin, xmx, linestyles='dashed')\r\n",
        "  plt.xlabel('Depth Denoise Layer was Inserted')\r\n",
        "  plt.ylabel('Accuracy')\r\n",
        "  plt.title(eps)\r\n",
        "  plt.xlim(xmin, xmx)\r\n",
        "  plt.xticks(np.arange(xmin,xmx+1))\r\n",
        "  plt.legend(['Modified Network', 'Baseline'])\r\n",
        "  plt.savefig(out_dir+'FGSM_ACC_'+eps+'denoise_all_nogrid.eps', format='eps')\r\n",
        "  plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BPYVkWH_lMa"
      },
      "source": [
        "## Test Accuracy of Best Multi Layer Insertion For Corresponding Epsilon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1PPYnv3_9nK"
      },
      "source": [
        "# Load accuracies from results file\n",
        "eps_cnt = 5\n",
        "layer_cnt = 21\n",
        "with open('/content/drive/MyDrive/DeepLearningFinal/results/FGSM_All_Layer_Accuracy.txt','r') as file1:\n",
        "  for jj in range(0,eps_cnt):\n",
        "    # iterate over each epsilon\n",
        "    accs = []\n",
        "    layer_acc = {}\n",
        "    for ii in range(0,layer_cnt+2):\n",
        "      line = file1.readline()\n",
        "      if ii == 0:\n",
        "        eps = float(line.split('_')[1])\n",
        "      else:\n",
        "        layer_num = int(line.split(' ')[0])\n",
        "        acc = (float(line.split(' ')[1]))\n",
        "        layer_acc[layer_num] = acc\n",
        "    accs_by_eps[eps] = layer_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1qqv22AIYOr"
      },
      "source": [
        "# get list of layers in desc order of acc per epsilon\n",
        "best_layers_by_eps = {}\n",
        "for eps,layer_accs in accs_by_eps.items():\n",
        "  baseline = layer_accs[0]\n",
        "  # remove baseline\n",
        "  layer_accs = {key:layer_accs[key] for key in range(1,22)}\n",
        "  # sort layers by acc\n",
        "  sorted_layer_accs = sorted(layer_accs.items(), key =lambda x: x[1],reverse=True)\n",
        "  sorted_layer_nums = []\n",
        "  for layer_num,acc in sorted_layer_accs:\n",
        "    sorted_layer_nums.append(layer_num)\n",
        "  best_layers_by_eps[eps] = sorted_layer_nums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVgCHlUEWzkb"
      },
      "source": [
        "# Create a different list of models for each eps to test\n",
        "\n",
        "# max number of layers to insert\n",
        "layer_insertion_cap = 5\n",
        "models_list_by_eps = {}\n",
        "for eps,layers in best_layers_by_eps.items():\n",
        "  model_insert_positions = []\n",
        "  for ii in range(0,layer_insertion_cap+1):\n",
        "    selected_layers = layers[0:ii]\n",
        "    model_insert_positions.append(selected_layers)\n",
        "  model_insert_pos_by_eps[eps] = model_insert_positions\n",
        "  model_insert_names = []\n",
        "  for positions in model_insert_positions:\n",
        "    model_insert_names.append([vgg_names[i] for i in positions])\n",
        "  models_list = []\n",
        "  for lst in model_insert_names:\n",
        "    models_list.append(split_model(vgg, lst))\n",
        "  models_list_by_eps[eps] = models_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GxuOQUaeR5Y"
      },
      "source": [
        "models_list_by_eps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZm_n78UVoRo"
      },
      "source": [
        "## ACCURACY TESTING LOOP\n",
        "N_denoise = 50\n",
        "TestingLoops = 5\n",
        "noise_sigma = 100\n",
        "\n",
        "eps_list = ['eps_0.0', 'eps_0.1', 'eps_0.3', 'eps_0.5', 'eps_1.0']\n",
        "\n",
        "#dir = \"/content/drive/MyDrive/DeepLearning/DeepLearningFinal/adversarial_images/fgsm/\"\n",
        "dir =  \"/content/drive/MyDrive/DeepLearningFinal/adversarial_images/fgsm/\"\n",
        "\n",
        "acc_save = {}\n",
        "for eps in eps_list:\n",
        "  eps_val = float(eps.split(\"_\")[1])\n",
        "  #Initialize epselon dictionary\n",
        "  acc_save[eps] = {}\n",
        "  #Get directory\n",
        "  select_dir = dir+eps\n",
        "  #Import Images\n",
        "  val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    select_dir,\n",
        "    seed = 123,\n",
        "    shuffle=True,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=1)\n",
        "  class_names = val_ds.class_names\n",
        "\n",
        "  # extract images and labels from validation set and convert human-readable labels\n",
        "  # to ImageNet class IDs\n",
        "  images, labels = [], []\n",
        "  class_ids = {'bikes': 671, 'ships': 510}\n",
        "  for img, label in val_ds:\n",
        "    class_id = class_ids[class_names[int(label)]]\n",
        "    labels.append(class_id)\n",
        "    images.append(img)\n",
        "\n",
        "  # get list of models for current eps\n",
        "  models_list = models_list_by_eps[eps_val]\n",
        "  #Iterate over the models we inserted the layers in \n",
        "  for ii, mdl in enumerate(models_list):\n",
        "    #Count the number of correct predictions\n",
        "    correct_prediction = []\n",
        "    #Run testing a couple times on the same set of images so we average out some of the randomness\n",
        "    for loop in range(TestingLoops):\n",
        "      #Loop over images\n",
        "      for idx, im in enumerate(images):\n",
        "        L = len(mdl)\n",
        "        inter = im\n",
        "        #Loop over models\n",
        "        for i, block in enumerate(mdl):\n",
        "          inter = block(inter)\n",
        "          if i != L-1:\n",
        "            #Costum layer insertion:\n",
        "\n",
        "            shp = inter.shape\n",
        "            intermediate = inter.numpy()\n",
        "            inter = denoise_all_channels(intermediate, shp)\n",
        "          else:\n",
        "            out = np.argmax(inter)\n",
        "        correct_prediction.append(labels[idx] == int(out))\n",
        "\n",
        "    #Once all the images are out, calculate the percent accuracy:\n",
        "    acc_save[eps][str(ii)] = np.mean(correct_prediction)\n",
        "    ii +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXkeXQ4iL6Hm"
      },
      "source": [
        "## Save and Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NPwfd-AIdIW"
      },
      "source": [
        "# save accuracy to file\n",
        "with open('/content/drive/MyDrive/DeepLearningFinal/results/fgsm_accuracy.txt','a') as file:\n",
        "  file.write(\"Multi-layer Insertion\\n\")\n",
        "  file.write(\"Denoise Function: All\\n\")\n",
        "  file.write(\"Testing Loops: \" + str(TestingLoops) + \"\\n\")\n",
        "  file.write(\"N_denoise: \" + str(N_denoise) + \"\\n\")\n",
        "  file.write(\"Noise_sigma: \" + str(noise_sigma) + \"\\n\")\n",
        "  for eps,model in acc_save.items():\n",
        "    file.write(eps + \"\\n\")\n",
        "    for model_name,acc in model.items():\n",
        "      file.write(model_name + \" \" + str(acc) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18eWER7LItMd"
      },
      "source": [
        "out_dir = '/content/drive/MyDrive/DeepLearningFinal/results/Multi-layer Insertion Test/'\n",
        "\n",
        "for eps in eps_list:\n",
        "  plt.figure(figsize = (8,4))\n",
        "  accs = []\n",
        "  for acc in acc_save[eps].values():\n",
        "    accs.append(acc)\n",
        "    \n",
        "  plt.plot(np.arange(1,6), accs[1:])\n",
        "  xmin, xmx = plt.xlim()\n",
        "  plt.hlines(accs[0], xmin, xmx, linestyles='dashed')\n",
        "  plt.xlabel('Number of Layers Inserted')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title(eps)\n",
        "  plt.xlim(xmin, xmx)\n",
        "  plt.xticks(np.arange(1,6))\n",
        "  plt.legend(['Modified Network', 'Baseline'])\n",
        "  plt.savefig(out_dir+'FGSM_ACC_'+eps+'.eps', format='eps')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_ZEU2Eb-Mwk"
      },
      "source": [
        "# DeepFool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG544Jvt-PGc"
      },
      "source": [
        "## Test Accuracy of Each Single Layer Insertion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFQqs8-5LbY8"
      },
      "source": [
        "## ACCURACY TESTING LOOP WITHOUT EPSILONS\n",
        "N_denoise = 50\n",
        "TestingLoops = 5\n",
        "noise_sigma = 100\n",
        "\n",
        "dir = \"/content/drive/MyDrive/DeepLearning/DeepLearningFinal/adversarial_images/deepfoolL2/\"\n",
        "dir =  \"/content/drive/MyDrive/DeepLearningFinal/adversarial_images/deepfoolL2/\"\n",
        "\n",
        "#Initialize epselon dictionary\n",
        "acc_save = {}\n",
        "#Get directory\n",
        "select_dir = dir\n",
        "#Import Images\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  select_dir,\n",
        "  seed = 123,\n",
        "  shuffle=True,\n",
        "  image_size=(224, 224),\n",
        "  batch_size=1)\n",
        "class_names = val_ds.class_names\n",
        "\n",
        "# extract images and labels from validation set and convert human-readable labels\n",
        "# to ImageNet class IDs\n",
        "images, labels = [], []\n",
        "class_ids = {'bikes': 671, 'ships': 510}\n",
        "for img, label in val_ds:\n",
        "  class_id = class_ids[class_names[int(label)]]\n",
        "  labels.append(class_id)\n",
        "  images.append(img)\n",
        "\n",
        "#Iterate over the models we inserted the layers in \n",
        "for ii, mdl in enumerate(models_list):\n",
        "  #Count the number of correct predictions\n",
        "  correct_prediction = []\n",
        "  #Run testing a couple times on the same set of images so we average out some of the randomness\n",
        "  for loop in range(TestingLoops):\n",
        "    #Loop over images\n",
        "    for idx, im in enumerate(images):\n",
        "      L = len(mdl)\n",
        "      inter = im\n",
        "      #Loop over models\n",
        "      for i, block in enumerate(mdl):\n",
        "        inter = block(inter)\n",
        "        if i != L-1:\n",
        "          #Costum layer insertion:\n",
        "\n",
        "          shp = inter.shape\n",
        "          intermediate = inter.numpy()\n",
        "          inter = denoise_all_channels(intermediate, shp)\n",
        "        else:\n",
        "          out = np.argmax(inter)\n",
        "      correct_prediction.append(labels[idx] == int(out))\n",
        "\n",
        "\n",
        "  #Once all the images are out, calculate the percent accuracy:\n",
        "  acc_save[str(ii)] = np.mean(correct_prediction)\n",
        "  ii +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XvQ-AHifOZ2"
      },
      "source": [
        "## Save and Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbHNI9vkO8L6"
      },
      "source": [
        "#Save Accuracy to files\n",
        "with open('/content/drive/MyDrive/DeepLearningFinal/results/deepfool_accuracy.txt','a') as file:\n",
        "  file.write(\"Denoise type: All \\n\")\n",
        "  file.write(\"Testing Loops: \" + str(TestingLoops) + \"\\n\")\n",
        "  file.write(\"N_denoise: \" + str(N_denoise) + \"\\n\")\n",
        "  file.write(\"Noise_sigma: \" + str(noise_sigma) + \"\\n\")\n",
        "  for model_name,acc in acc_save.items():\n",
        "    file.write(model_name + \" \" + str(acc) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCBSi_Dl1oUh"
      },
      "source": [
        "# Plot accuracies\n",
        "layer_nums = np.array(model_insert_positions[1:])\n",
        "#out_dir = '/content/drive/MyDrive/DeepLearning/DeepLearningFinal/results/'\n",
        "out_dir = '/content/drive/MyDrive/DeepLearningFinal/results/'\n",
        "\n",
        "plt.figure(figsize = (8,4))\n",
        "accs = []\n",
        "for acc in acc_save.values():\n",
        "  accs.append(acc)\n",
        "  \n",
        "plt.plot(layer_nums, accs[1:])\n",
        "xmin, xmx = plt.xlim()\n",
        "plt.hlines(accs[0], xmin, xmx, linestyles='dashed')\n",
        "plt.xlabel('Depth Denoise Layer was Inserted')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(\"Deepfool\")\n",
        "plt.xlim(xmin, xmx)\n",
        "plt.xticks(np.arange(xmin,xmx+1))\n",
        "plt.legend(['Modified Network', 'Baseline'])\n",
        "plt.grid()\n",
        "plt.savefig(out_dir+'DeepFool_ACC_denoise_all_grid.eps', format='eps')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCp7KBllMIO8"
      },
      "source": [
        "## Test Accuracy of Best Multi Layer Insertion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOcsmTrAMGWq"
      },
      "source": [
        "# Load accuracies from results file\n",
        "layer_cnt = 21\n",
        "with open('/content/drive/MyDrive/DeepLearningFinal/results/DeepFool_all_layer_acc.txt','r') as file1:\n",
        "  layer_accs = {}\n",
        "  for ii in range(0,layer_cnt+1):\n",
        "    line = file1.readline()\n",
        "    layer_num = int(line.split(' ')[0])\n",
        "    acc = (float(line.split(' ')[1]))\n",
        "    layer_accs[layer_num] = acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vsx0nIuNOWv"
      },
      "source": [
        "# get list of layers in desc order of acc\n",
        "best_layers = {}\n",
        "baseline = layer_accs[0]\n",
        "# remove baseline\n",
        "layer_accs = {key:layer_accs[key] for key in range(1,22)}\n",
        "# sort layers by acc\n",
        "sorted_layer_accs = sorted(layer_accs.items(), key =lambda x: x[1],reverse=True)\n",
        "sorted_layer_nums = []\n",
        "for layer_num,acc in sorted_layer_accs:\n",
        "  sorted_layer_nums.append(layer_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwR3nNEyNegm"
      },
      "source": [
        "# Create a list of models to test\n",
        "\n",
        "# max number of layers to insert\n",
        "layer_insertion_cap = 5\n",
        "models_list = {}\n",
        "model_insert_positions = []\n",
        "for ii in range(0,layer_insertion_cap+1):\n",
        "  selected_layers = sorted_layer_nums[0:ii]\n",
        "  model_insert_positions.append(selected_layers)\n",
        "model_insert_names = []\n",
        "for positions in model_insert_positions:\n",
        "  model_insert_names.append([vgg_names[i] for i in positions])\n",
        "models_list = []\n",
        "for lst in model_insert_names:\n",
        "  models_list.append(split_model(vgg, lst))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0esCnJ0ZOurQ"
      },
      "source": [
        "## ACCURACY TESTING LOOP\n",
        "N_denoise = 50\n",
        "TestingLoops = 5\n",
        "noise_sigma = 100\n",
        "\n",
        "# dir = \"/content/drive/MyDrive/DeepLearning/DeepLearningFinal/adversarial_images/deepfoolL2/\"\n",
        "dir =  \"/content/drive/MyDrive/DeepLearningFinal/adversarial_images/deepfoolL2/\"\n",
        "\n",
        "#Initialize epselon dictionary\n",
        "acc_save = {}\n",
        "#Get directory\n",
        "select_dir = dir\n",
        "#Import Images\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  select_dir,\n",
        "  seed = 123,\n",
        "  shuffle=True,\n",
        "  image_size=(224, 224),\n",
        "  batch_size=1)\n",
        "class_names = val_ds.class_names\n",
        "\n",
        "# extract images and labels from validation set and convert human-readable labels\n",
        "# to ImageNet class IDs\n",
        "images, labels = [], []\n",
        "class_ids = {'bikes': 671, 'ships': 510}\n",
        "for img, label in val_ds:\n",
        "  class_id = class_ids[class_names[int(label)]]\n",
        "  labels.append(class_id)\n",
        "  images.append(img)\n",
        "\n",
        "#Iterate over the models we inserted the layers in \n",
        "for ii, mdl in enumerate(models_list):\n",
        "  #Count the number of correct predictions\n",
        "  correct_prediction = []\n",
        "  #Run testing a couple times on the same set of images so we average out some of the randomness\n",
        "  for loop in range(TestingLoops):\n",
        "    #Loop over images\n",
        "    for idx, im in enumerate(images):\n",
        "      L = len(mdl)\n",
        "      inter = im\n",
        "      #Loop over models\n",
        "      for i, block in enumerate(mdl):\n",
        "        inter = block(inter)\n",
        "        if i != L-1:\n",
        "          #Costum layer insertion:\n",
        "\n",
        "          shp = inter.shape\n",
        "          intermediate = inter.numpy()\n",
        "          inter = denoise_all_channels(intermediate, shp)\n",
        "        else:\n",
        "          out = np.argmax(inter)\n",
        "      correct_prediction.append(labels[idx] == int(out))\n",
        "\n",
        "\n",
        "  #Once all the images are out, calculate the percent accuracy:\n",
        "  acc_save[str(ii)] = np.mean(correct_prediction)\n",
        "  ii +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khJex_oCPXKD"
      },
      "source": [
        "#Save Accuracy to files\n",
        "with open('/content/drive/MyDrive/DeepLearningFinal/results/deepfool_accuracy.txt','a') as file:\n",
        "  file.write(\"Multi-layer insertion\\n\")\n",
        "  file.write(\"Denoise type: All \\n\")\n",
        "  file.write(\"Testing Loops: \" + str(TestingLoops) + \"\\n\")\n",
        "  file.write(\"N_denoise: \" + str(N_denoise) + \"\\n\")\n",
        "  file.write(\"Noise_sigma: \" + str(noise_sigma) + \"\\n\")\n",
        "  for model_name,acc in acc_save.items():\n",
        "    file.write(model_name + \" \" + str(acc) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpP_nbfTPiMY"
      },
      "source": [
        "# Plot accuracies\n",
        "\n",
        "#out_dir = '/content/drive/MyDrive/DeepLearning/DeepLearningFinal/results/'\n",
        "out_dir = '/content/drive/MyDrive/DeepLearningFinal/results/Multi-layer Insertion Test/'\n",
        "\n",
        "plt.figure(figsize = (8,4))\n",
        "accs = []\n",
        "for acc in acc_save.values():\n",
        "  accs.append(acc)\n",
        "plt.plot(np.arange(1,6), accs[1:])\n",
        "xmin, xmx = plt.xlim()\n",
        "plt.hlines(accs[0], xmin, xmx, linestyles='dashed')\n",
        "plt.xlabel('Number of Layers Inserted')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(\"Deepfool\")\n",
        "plt.xlim(xmin, xmx)\n",
        "plt.xticks(np.arange(1,6))\n",
        "plt.legend(['Modified Network', 'Baseline'])\n",
        "plt.savefig(out_dir+'DeepFool_ACC_multi-layer.eps', format='eps')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}